{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**交差エントロビー誤差**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.3025840929945454\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "\n",
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y1 = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "y2 = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "\n",
    "print(f'{cross_entropy_error(y1, t)}')\n",
    "print(f'{cross_entropy_error(y2, t)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ミニバッチ学習**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5746  9707  5413 21196 18949 38859 31959 43431 36018 36155]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(X_train, t_train), (X_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "# print(X_train.shape)\n",
    "# print(t_train.shape)\n",
    "\n",
    "train_size = X_train.shape[0] # 学習データの個数\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) # バッチサイズ分データのインデックスをランダムに取り出し\n",
    "print(batch_mask)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数値微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**中心差分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**偏微分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数の定義\n",
    "# f(x0, x1) = x0 ** 2 + x1 ** 2\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x0 = 3, x = 4のときのx0に対する偏微分を求めよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.00000000000378\n"
     ]
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    # print(x0 * x0 + 4.0 ** 2.0)\n",
    "    return x0 * x0 + 4.0 ** 2.0\n",
    "\n",
    "x0_grad = numerical_diff(function_tmp1, 3.0)\n",
    "print(x0_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x0 = 3, x = 4のときのx1に対する偏微分を求めよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "def function_tmp2(x1):\n",
    "    # print(x0 * x0 + 4.0 ** 2.0)\n",
    "    return 3.0 ** 2.0 + x1 * x1\n",
    "\n",
    "x0_grad = numerical_diff(function_tmp1, 4.0)\n",
    "print(x0_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**勾配の計算**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    \"\"\"勾配を求める\n",
    "    \n",
    "    [Parameter]\n",
    "    ---------\n",
    "    f : 最小値を求める関数\n",
    "    x : x0, x1パラメータの初期値\n",
    "    \n",
    "    [Return] \n",
    "    ---------\n",
    "    grad : 勾配\n",
    "    \"\"\"\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros(x.size) # 勾配を0で初期化\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # print(f'{idx + 1} 個目の重みの勾配を計算 {tmp_val}')\n",
    "        \n",
    "        # f(x + h)の計算\n",
    "        x[idx] = tmp_val + h \n",
    "        fxh1 = f(x)\n",
    "    \n",
    "        # f(x - h)の計算\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        # (f(x + h) - f(x - h)) / (2 * h)の計算\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "\n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        \n",
    "    # print(grad)\n",
    "    return grad\n",
    "\n",
    "numerical_gradient(function_2, np.array([0.3, 0.4]))\n",
    "numerical_gradient(function_2, np.array([0.0, 2.0]))\n",
    "numerical_gradient(function_2, np.array([3.0, 0.0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**勾配降下法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1回目: 勾配[-6.  8.]\n",
      "Step1回目: パラメータ[-2.94  3.92]\n",
      "Step2回目: 勾配[-5.88  7.84]\n",
      "Step2回目: パラメータ[-2.8812  3.8416]\n",
      "Step3回目: 勾配[-5.7624  7.6832]\n",
      "Step3回目: パラメータ[-2.823576  3.764768]\n",
      "Step4回目: 勾配[-5.647152  7.529536]\n",
      "Step4回目: パラメータ[-2.76710448  3.68947264]\n",
      "Step5回目: 勾配[-5.53420896  7.37894528]\n",
      "Step5回目: パラメータ[-2.71176239  3.61568319]\n",
      "Step10回目: 勾配[-5.00248657  6.6699821 ]\n",
      "Step10回目: パラメータ[-2.45121842  3.26829123]\n",
      "Step30回目: 勾配[-3.33969991  4.45293322]\n",
      "Step30回目: パラメータ[-1.63645296  2.18193728]\n",
      "Step50回目: 勾配[-2.22961029  2.97281371]\n",
      "Step50回目: パラメータ[-1.09250904  1.45667872]\n",
      "Step80回目: 勾配[-1.21621745  1.62162327]\n",
      "Step80回目: パラメータ[-0.59594655  0.7945954 ]\n",
      "Step100回目: 勾配[-0.81195646  1.08260862]\n",
      "Step100回目: パラメータ[-0.39785867  0.53047822]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.39785867,  0.53047822])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function_2関数の最小値を勾配降下法を使って探す\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.1, step_num=100):\n",
    "    \"\"\"勾配降下法\n",
    "    \n",
    "    --------\n",
    "    [Parameter]\n",
    "    f       : 最小値を求める関数\n",
    "    init_x  : パラメータx0,x1の初期値\n",
    "    lr      : 学習率\n",
    "    step_num: パラメータを更新する回数\n",
    "    \n",
    "    -------\n",
    "    [Return]\n",
    "    x       : 更新したパラメータの最終結果\n",
    "    \"\"\"\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        \n",
    "        if i+1 in [1, 2, 3, 4, 5, 10, 30, 50, 80, 100]:\n",
    "            print(f'Step{i + 1}回目: 勾配{grad}')\n",
    "            print(f'Step{i + 1}回目: パラメータ{x}')\n",
    "        \n",
    "    return x\n",
    "\n",
    "gradient_descent(function_2, init_x, lr=0.01, step_num=100)\n",
    "# gradient_descent(function_2, init_x, lr=10.0, step_num=100) # 学習率が大きすぎる\n",
    "# gradient_descent(function_2, init_x, lr=1e-10, step_num=100) # 学習率が小さすぎる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ニューラルネットワークに対する勾配**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"重みを初期化(2行3列)\n",
    "        \"\"\"\n",
    "        self.W = np.random.randn(2, 3)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"入力値を受け取り、各ノードの加重和を計算\n",
    "        \"\"\"\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数を求める\n",
    "        \n",
    "        [Parameter]\n",
    "        -----------\n",
    "        x : 入力値\n",
    "        t : 正解ラベル\n",
    "        \n",
    "        [Return]\n",
    "        -----------\n",
    "        loss : 損失関数\n",
    "        \"\"\"\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8562116818445138"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "# print(net.W) # 重み\n",
    "x = np.array([0.6, 0.9]) # 入力値\n",
    "p = net.predict(x)\n",
    "# print(p) # 加重和\n",
    "# print(np.argmax(p)) # 最大値のインデックス\n",
    "\n",
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03895352  0.46728812 -0.50624164]\n",
      " [ 0.05843028  0.70093218 -0.75936247]]\n",
      "\n",
      "重み [[-0.40257729  0.17859138  0.96420011]\n",
      " [-0.36479206  2.00840306 -0.30003535]]\n"
     ]
    }
   ],
   "source": [
    "# f = lambda w: net.loss(x, t)\n",
    "def f(w):\n",
    "    \n",
    "    return net.loss(x, t)\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "dw = numerical_gradient(f, net.W) # 勾配の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
